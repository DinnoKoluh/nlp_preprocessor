{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from NLP import *\n",
    "from SentenceSplitterML import *\n",
    "from TokenizerML import *\n",
    "from lexicons import *\n",
    "import nltk as nltk\n",
    "from Parser import *\n",
    "from grammar import *\n",
    "import daft\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and tokenizing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'p2_data1'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.sentence_split()\n",
    "sentences = tokenizer.sentences\n",
    "for sentence in sentences:\n",
    "    t = NLP(sentence)\n",
    "    t.tokenize()\n",
    "    tokens = t.dirty_tokens\n",
    "    print(nltk.pos_tag(tokens[0:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Get', 'VB'), ('the', 'DT'), ('flight', 'NN'), ('through', 'IN'), ('Huston', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = nltk.tokenize.word_tokenize(\"Get the flight through Huston\")\n",
    "tag = nltk.pos_tag(text)\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Get', 'VB'), ('the', 'DT'), ('flight', 'NN'), ('through', 'IN'), ('Huston', 'NNP')]\n",
      "[['Det', 'Nominal'], ['Verb', 'NP'], ['Verb', 'NP'], ['Prep', 'NP'], ['Nominal', 'PP'], ['Det', 'Nominal'], ['Verb', 'NP'], ['Verb', 'NP'], ['VP', 'PP'], ['VP', 'PP']]\n",
      "[[], [], [[(0, 0, 'Verb'), (1, 2, 'NP'), 'S'], [(0, 0, 'Verb'), (1, 2, 'NP'), 'VP']], [], [[(0, 0, 'Verb'), (1, 4, 'NP'), 'S'], [(0, 0, 'Verb'), (1, 4, 'NP'), 'VP'], [(0, 2, 'VP'), (3, 4, 'PP'), 'S'], [(0, 2, 'VP'), (3, 4, 'PP'), 'VP']]]\n",
      "[[], [], [[(1, 1, 'Det'), (2, 2, 'Nominal'), 'NP']], [], [[(1, 1, 'Det'), (2, 4, 'Nominal'), 'NP']]]\n",
      "[[], [], [], [], [[(2, 2, 'Nominal'), (3, 4, 'PP'), 'Nominal']]]\n",
      "[[], [], [], [], [[(3, 3, 'Prep'), (4, 4, 'NP'), 'PP']]]\n",
      "[[], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "text = \"I bought a red gift for my friend yesterday\"\n",
    "text = \"Get the flight through Huston\"\n",
    "#text = \"The flight includes a meal\"\n",
    "\n",
    "p = Parser(text)\n",
    "print(p.parsed_text)\n",
    "table, links, my_table = p.parse()\n",
    "p.save_table(\"data/parse_table.txt\")\n",
    "#p.make_tree(my_table)\n",
    "print(links)\n",
    "for tab in my_table:\n",
    "    print(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for i in range(len(links)):\n",
    "    if i == 0:\n",
    "        G.add_node(links[i][0], label=\"Dinno\", )\n",
    "        G.add_edges_from([(links[i][0], links[i][1][0]), (links[i][0], links[i][1][1])])\n",
    "    else:\n",
    "        G.add_edges_from([(links[i][0], links[i][1][0]), (links[i][0], links[i][1][1])])\n",
    "nx.draw(G, with_labels = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "H = 10\n",
    "pgm = daft.PGM(shape=[L, H])\n",
    "\n",
    "y = H\n",
    "x = L/2\n",
    "pgm.add_node(daft.Node('0', links[0][0], L/2, H))\n",
    "for i in range(len(links)):\n",
    "    y = y - 1\n",
    "    for j in range(len(links[i])):\n",
    "        pgm.add_node(daft.Node(str(i)+'_'+str(j), links[i][1][j], x+j+i, y))\n",
    "        if i == 0:\n",
    "            pgm.add_edge('0', str(i)+'_'+str(j))\n",
    "        else:\n",
    "            pgm.add_edge(str(i-1)+'_'+str(1), str(i)+'_'+str(j))\n",
    "\n",
    "pgm.render()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14664053d37e575f017670323f8571077f235bd162d1842d904390d31d3de60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
