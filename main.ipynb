{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from NLP import *\n",
    "from lexicons import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "print(tokenizer.text)\n",
    "print(tokenizer.rough_tokens)\n",
    "print(tokenizer.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "#tokenizer.tokenize()\n",
    "tokenizer.sentence_split()\n",
    "sentences = tokenizer.sentences\n",
    "print(text)\n",
    "for sentence in sentences:\n",
    "    print([sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabularies and word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "tokenizer.expand_clitics()\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "# print(tokenizer.word_frequency)\n",
    "tokenizer.clean_vocabulary()\n",
    "print(tokenizer.vocabulary)\n",
    "# print(tokenizer.word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''\n",
      "['It', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'people', 'cease', 'pirating', 'files', 'when', 'those', 'same', 'people', 'are', 'not', 'paid', 'for', 'their', 'participation', 'in', 'very', 'lucrative', 'network', 'schemes', '.', 'Ordinary', 'people', 'are', 'relentlessly', 'spied', 'on', ',', 'and', 'not', 'compensated', 'for', 'information', 'taken', 'from', 'them', '.', 'While', 'I', 'would', 'like', 'to', 'see', 'everyone', 'eventually', 'pay', 'for', 'music', 'and', 'the', 'like', ',', 'I', 'would', 'not', 'ask', 'for', 'it', 'until', 'there', 'is', 'reciprocity', '.']\n",
      "[\"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes.\"]\n",
      "['Ordinary people are relentlessly spied on, and not compensated for information taken from them.']\n",
      "[\"While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\"]\n",
      "['I', 'It', 'Ordinary', 'While', 'and', 'are', 'ask', 'be', 'cease', 'compensated', 'demand', 'eventually', 'everyone', 'files', 'for', 'from', 'in', 'information', 'is', 'it', 'like', 'lucrative', 'music', 'network', 'not', 'on', 'paid', 'participation', 'pay', 'people', 'pirating', 'reciprocity', 'relentlessly', 'same', 'schemes', 'see', 'spied', 'taken', 'that', 'the', 'their', 'them', 'there', 'those', 'to', 'unfair', 'until', 'very', 'when', 'would']\n",
      "{'for': 4, 'would': 3, 'people': 3, 'not': 3, 'to': 2, 'are': 2, 'and': 2, 'I': 2, 'like': 2, 'It': 1, 'be': 1, 'unfair': 1, 'demand': 1, 'that': 1, 'cease': 1, 'pirating': 1, 'files': 1, 'when': 1, 'those': 1, 'same': 1, 'paid': 1, 'their': 1, 'participation': 1, 'in': 1, 'very': 1, 'lucrative': 1, 'network': 1, 'schemes': 1, 'Ordinary': 1, 'relentlessly': 1, 'spied': 1, 'on': 1, 'compensated': 1, 'information': 1, 'taken': 1, 'from': 1, 'them': 1, 'While': 1, 'see': 1, 'everyone': 1, 'eventually': 1, 'pay': 1, 'music': 1, 'the': 1, 'ask': 1, 'it': 1, 'until': 1, 'there': 1, 'is': 1, 'reciprocity': 1}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(\"\")\n",
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.pipeline()\n",
    "\n",
    "print(tokenizer.tokens)\n",
    "for s in tokenizer.sentences:\n",
    "    print([s])\n",
    "print(tokenizer.vocabulary)\n",
    "print(tokenizer.word_frequency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp_p1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14664053d37e575f017670323f8571077f235bd162d1842d904390d31d3de60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
