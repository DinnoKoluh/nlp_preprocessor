{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from funs import *\n",
    "from NLP import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['din', ',', 'no', 'e.g.', 'you']\n"
     ]
    }
   ],
   "source": [
    "ex = 'din,noe.g.you'\n",
    "t = NLP([])\n",
    "print(t.split_by_string(ex, 'e.g.', [',']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt1'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "print(tokenizer.text)\n",
    "print(tokenizer.rough_tokens)\n",
    "print(tokenizer.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt3'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "#tokenizer.tokenize()\n",
    "tokenizer.sentence_split()\n",
    "sentences = tokenizer.sentences\n",
    "print(text)\n",
    "for sentence in sentences:\n",
    "    print([sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabularies and word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '$45,000,055', \"'\", '(', ')', ',', '.', '..', '0.5M', '01/20/2020', '150', ':', ';', '==', '?', 'A', 'Assistant', 'CALO', 'Cognitive', 'Commission', 'Doe', 'Energy', 'Enron', 'Federal', 'Gervasio', 'I', 'If', 'Invalid', 'It', 'John', 'K.', 'Kaelbling', 'Learns', 'Leslie', 'MIT', 'Machiaveli', 'Mary', 'Melinda', 'Mr.', 'Organizes', 'Project', 'Regulatory', 'SRI', 'Smith', 'The', 'This', 'a', 'about', 'affected', 'am', 'and', 'answer', 'as', 'ask', 'at', 'attachments', 'available', 'been', 'by', 'collected', 'contains', 'data', 'dataset', 'deleted', 'dino', \"don't\", 'due', 'during', 'each', 'effort', 'email', 'employees', 'feel', 'folders', 'folks', 'form', 'format', 'from', 'get', 'hello', 'https://www.naxi.rs/hype', 'i.e.', 'in', 'include', 'into', 'investigation', 'is', 'it', 'its', 'just', 'know', 'koluh_dinno@gmail.com', 'later', 'like', 'management', 'me', 'messages', 'mindme', 'mostly', 'no', 'no_address@enron.com', 'not', 'notably', 'number', 'of', 'or', 'organized', 'parse-able', 'part', 'please', 'possible', 'posted', 'prepared', 'problems', 'public', 'purchased', 'question', 'recipient', 'redaction', 'requests', 'senior', 'slighted', 'some', 'specified', 'thanks', 'that', 'the', 'them', 'to', 'unable', 'user@enron.com.', 'users', 'was', 'web', 'week', 'when', 'whenever', 'which', 'worked', 'you']\n",
      "{',': 19, '.': 13, 'and': 8, '\"': 6, 'the': 5, 'of': 5, 'to': 5, \"'\": 4, 'I': 4, \"don't\": 4, 'dataset': 3, 'was': 3, 'by': 3, '(': 3, 'that': 3, ')': 3, 'about': 3, 'The': 3, 'is': 3, 'a': 3, 'This': 2, 'Mr.': 2, 'A': 2, 'from': 2, 'mostly': 2, '?': 2, 'at': 2, 'problems': 2, 'me': 2, 'recipient': 2, 'specified': 2, 'answer': 2, 'collected': 1, 'prepared': 1, 'Machiaveli': 1, 'CALO': 1, 'Project': 1, 'Cognitive': 1, 'Assistant': 1, 'Learns': 1, 'Organizes': 1, 'It': 1, 'contains': 1, 'data': 1, '150': 1, 'users': 1, 'senior': 1, 'management': 1, 'Enron': 1, 'organized': 1, 'into': 1, 'folders': 1, '0.5M': 1, 'messages': 1, 'public': 1, 'posted': 1, 'web': 1, 'Federal': 1, 'Energy': 1, 'Regulatory': 1, 'Commission': 1, 'during': 1, 'its': 1, 'investigation': 1, 'email': 1, 'later': 1, 'purchased': 1, 'Leslie': 1, 'Kaelbling': 1, 'MIT': 1, 'number': 1, 'folks': 1, 'SRI': 1, 'notably': 1, 'Melinda': 1, 'Gervasio': 1, 'worked': 1, 'it': 1, 'thanks': 1, 'them': 1, 'not': 1, 'available': 1, 'include': 1, 'attachments': 1, 'been': 1, 'deleted': 1, 'as': 1, 'part': 1, 'redaction': 1, 'effort': 1, 'due': 1, 'requests': 1, 'affected': 1, 'employees': 1, 'Invalid': 1, 'form': 1, 'user@enron.com.': 1, 'whenever': 1, 'possible': 1, '..': 1, 'i.e.': 1, 'in': 1, 'some': 1, 'parse-able': 1, 'format': 1, 'like': 1, 'Doe': 1, 'John': 1, 'or': 1, 'Mary': 1, 'K.': 1, 'Smith': 1, 'no_address@enron.com': 1, ':': 1, 'koluh_dinno@gmail.com': 1, ';': 1, 'when': 1, 'no': 1, 'get': 1, 'each': 1, 'week': 1, 'which': 1, 'am': 1, 'unable': 1, '$45,000,055': 1, 'just': 1, 'know': 1, 'If': 1, 'you': 1, 'ask': 1, 'question': 1, 'please': 1, 'mindme': 1, 'feel': 1, 'slighted': 1, 'dino': 1, '!': 1, '==': 1, 'hello': 1, '01/20/2020': 1, 'https://www.naxi.rs/hype': 1}\n",
      "['0.5M', '01/20/2020', '150', 'A', 'Assistant', 'CALO', 'Cognitive', 'Commission', 'Doe', 'Energy', 'Enron', 'Federal', 'Gervasio', 'I', 'If', 'Invalid', 'It', 'John', 'K.', 'Kaelbling', 'Learns', 'Leslie', 'MIT', 'Machiaveli', 'Mary', 'Melinda', 'Mr.', 'Organizes', 'Project', 'Regulatory', 'SRI', 'Smith', 'The', 'This', 'a', 'about', 'affected', 'am', 'and', 'answer', 'as', 'ask', 'at', 'attachments', 'available', 'been', 'by', 'collected', 'contains', 'data', 'dataset', 'deleted', 'dino', \"don't\", 'due', 'during', 'each', 'effort', 'email', 'employees', 'feel', 'folders', 'folks', 'form', 'format', 'from', 'get', 'hello', 'https://www.naxi.rs/hype', 'i.e.', 'in', 'include', 'into', 'investigation', 'is', 'it', 'its', 'just', 'know', 'koluh_dinno@gmail.com', 'later', 'like', 'management', 'me', 'messages', 'mindme', 'mostly', 'no', 'no_address@enron.com', 'not', 'notably', 'number', 'of', 'or', 'organized', 'parse-able', 'part', 'please', 'possible', 'posted', 'prepared', 'problems', 'public', 'purchased', 'question', 'recipient', 'redaction', 'requests', 'senior', 'slighted', 'some', 'specified', 'thanks', 'that', 'the', 'them', 'to', 'unable', 'user@enron.com.', 'users', 'was', 'web', 'week', 'when', 'whenever', 'which', 'worked', 'you']\n",
      "{'and': 8, 'the': 5, 'of': 5, 'to': 5, 'I': 4, \"don't\": 4, 'dataset': 3, 'was': 3, 'by': 3, 'that': 3, 'about': 3, 'The': 3, 'is': 3, 'a': 3, 'This': 2, 'Mr.': 2, 'A': 2, 'from': 2, 'mostly': 2, 'at': 2, 'problems': 2, 'me': 2, 'recipient': 2, 'specified': 2, 'answer': 2, 'collected': 1, 'prepared': 1, 'Machiaveli': 1, 'CALO': 1, 'Project': 1, 'Cognitive': 1, 'Assistant': 1, 'Learns': 1, 'Organizes': 1, 'It': 1, 'contains': 1, 'data': 1, '150': 1, 'users': 1, 'senior': 1, 'management': 1, 'Enron': 1, 'organized': 1, 'into': 1, 'folders': 1, '0.5M': 1, 'messages': 1, 'public': 1, 'posted': 1, 'web': 1, 'Federal': 1, 'Energy': 1, 'Regulatory': 1, 'Commission': 1, 'during': 1, 'its': 1, 'investigation': 1, 'email': 1, 'later': 1, 'purchased': 1, 'Leslie': 1, 'Kaelbling': 1, 'MIT': 1, 'number': 1, 'folks': 1, 'SRI': 1, 'notably': 1, 'Melinda': 1, 'Gervasio': 1, 'worked': 1, 'it': 1, 'thanks': 1, 'them': 1, 'not': 1, 'available': 1, 'include': 1, 'attachments': 1, 'been': 1, 'deleted': 1, 'as': 1, 'part': 1, 'redaction': 1, 'effort': 1, 'due': 1, 'requests': 1, 'affected': 1, 'employees': 1, 'Invalid': 1, 'form': 1, 'user@enron.com.': 1, 'whenever': 1, 'possible': 1, 'i.e.': 1, 'in': 1, 'some': 1, 'parse-able': 1, 'format': 1, 'like': 1, 'Doe': 1, 'John': 1, 'or': 1, 'Mary': 1, 'K.': 1, 'Smith': 1, 'no_address@enron.com': 1, 'koluh_dinno@gmail.com': 1, 'when': 1, 'no': 1, 'get': 1, 'each': 1, 'week': 1, 'which': 1, 'am': 1, 'unable': 1, 'just': 1, 'know': 1, 'If': 1, 'you': 1, 'ask': 1, 'question': 1, 'please': 1, 'mindme': 1, 'feel': 1, 'slighted': 1, 'dino': 1, 'hello': 1, '01/20/2020': 1, 'https://www.naxi.rs/hype': 1}\n"
     ]
    }
   ],
   "source": [
    "name = 'txt3'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.vocabulary)\n",
    "print(tokenizer.word_frequency)\n",
    "tokenizer.clean_vocabulary()\n",
    "print(tokenizer.vocabulary)\n",
    "print(tokenizer.word_frequency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66ad2a0fc99eef4a3bf89b375e9b9756c86124817c17e26a015d4a0d647e591d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
