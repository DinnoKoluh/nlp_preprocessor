{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from NLP import *\n",
    "from lexicons import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organ\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "word = 'organizes'\n",
    "print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "print(tokenizer.text)\n",
    "print(tokenizer.rough_tokens)\n",
    "print(tokenizer.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "#tokenizer.tokenize()\n",
    "tokenizer.sentence_split()\n",
    "sentences = tokenizer.sentences\n",
    "print(text)\n",
    "for sentence in sentences:\n",
    "    print([sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabularies and word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "tokenizer.expand_clitics()\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "# print(tokenizer.word_frequency)\n",
    "tokenizer.clean_vocabulary()\n",
    "print(tokenizer.vocabulary)\n",
    "# print(tokenizer.word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''\n",
      "['this', 'dataset', 'was', 'collected', 'and', 'prepared', 'by', 'mr.', 'machiaveli', 'the', 'calo', 'project', '(', 'a', 'cognitive', 'assistant', 'that', 'learns', 'and', 'organizes', ')', '.', 'it', 'contains', 'data', 'from', 'about', '150', 'users', ',', 'mostly', 'senior', 'management', 'of', 'enron', ',', 'organized', 'into', 'folders', '.', 'the', 'of', 'about', '0.5m', 'messages', '.', 'this', 'public', ',', 'and', 'posted', 'to', 'the', 'web', ',', 'by', 'the', 'federal', 'energy', 'regulatory', 'commission', 'during', 'its', 'investigation', '?', 'the', 'email', 'dataset', 'was', 'later', 'purchased', 'by', 'leslie', 'kaelbling', 'at', 'mit', ',', 'and', 'problems', '.', 'a', 'number', 'of', 'folks', 'at', 'sri', ',', 'notably', 'melinda', 'gervasio', ',', 'worked', 'problems', ',', 'and', 'it', 'is', 'thanks', 'to', 'them', '(', 'not', 'me', ')', 'that', 'the', 'dataset', 'is', 'available', '.', 'the', 'include', 'attachments', ',', 'and', 'been', 'deleted', '\"', 'as', 'part', 'of', 'a', 'redaction', 'effort', 'due', 'to', 'requests', 'from', 'affected', 'employees', '\"', '.', 'invalid', 'of', 'the', 'form', 'user@enron.com.', ',', \"'\", 'whenever', 'possible', '..', '(', 'i.e.', ',', 'recipient', 'is', 'specified', 'in', 'some', 'parse-able', 'format', 'like', '\"', 'doe', ',', 'john', '\"', 'or', '\"', 'mary', 'k.', 'smith', '\"', ')', 'and', 'to', ',', 'no_address@enron.com', ':', 'koluh_dinno@gmail.com', ';', 'when', 'no', 'recipient', 'was', 'specified', '.', '.', '.', 'i', 'get', 'a', 'each', 'week', ',', 'which', 'i', 'am', 'unable', 'to', 'answer', ',', '$45,000,055', 'mostly', 'that', 'i', 'just', 'do', 'not', 'know', 'about', '.', 'if', 'you', 'ask', 'me', 'a', 'question', 'and', 'i', 'do', 'not', 'answer', ',', 'please', \"'\", 'do', 'not', \"'\", ',', 'do', 'not', '.', 'mindme', \"'\", 'feel', 'slighted', '.', '.', 'dino', '!', '==', '?', 'hello', ',', 'mr.', ',', '01/20/2020', 'https://www.naxi.rs/hype', 'usa', 'u.s.a.', 'us']\n",
      "['thi', 'dataset', 'wa', 'collec', 'and', 'prepar', 'by', 'mr.', 'machiav', 'the', 'calo', 'project', '(', 'a', 'cognit', 'assist', 'that', 'learn', 'and', 'organ', ')', '.', 'it', 'contain', 'data', 'from', 'about', '150', 'user', ',', 'mostli', 'senior', 'manag', 'of', 'enron', ',', 'organ', 'into', 'folder', '.', 'the', 'of', 'about', '0.5m', 'messag', '.', 'thi', 'public', ',', 'and', 'pos', 'to', 'the', 'web', ',', 'by', 'the', 'feder', 'energi', 'regulatori', 'commiss', 'dure', 'it', 'investig', '?', 'the', 'email', 'dataset', 'wa', 'later', 'purchas', 'by', 'lesli', 'kaelbl', 'at', 'mit', ',', 'and', 'problem', '.', 'a', 'number', 'of', 'folk', 'at', 'sri', ',', 'notab', 'melinda', 'gervasio', ',', 'wor', 'problem', ',', 'and', 'it', 'i', 'thank', 'to', 'them', '(', 'not', 'me', ')', 'that', 'the', 'dataset', 'i', 'avail', '.', 'the', 'includ', 'attach', ',', 'and', 'been', 'delet', '\"', 'a', 'part', 'of', 'a', 'redact', 'effort', 'due', 'to', 'request', 'from', 'affec', 'employe', '\"', '.', 'invalid', 'of', 'the', 'form', 'user@enron.com.', ',', \"'\", 'whenev', 'possib', '..', '(', 'i.e.', ',', 'recipi', 'i', 'specifi', 'in', 'some', 'parse-', 'format', 'like', '\"', 'doe', ',', 'john', '\"', 'or', '\"', 'mari', 'k.', 'smith', '\"', ')', 'and', 'to', ',', 'no_address@enron.com', ':', 'koluh_dinno@gmail.com', ';', 'when', 'no', 'recipi', 'wa', 'specifi', '.', '.', '.', 'i', 'get', 'a', 'each', 'week', ',', 'which', 'i', 'am', 'unab', 'to', 'answer', ',', '$45,000,055', 'mostli', 'that', 'i', 'just', 'do', 'not', 'know', 'about', '.', 'if', 'you', 'ask', 'me', 'a', 'question', 'and', 'i', 'do', 'not', 'answer', ',', 'pleas', \"'\", 'do', 'not', \"'\", ',', 'do', 'not', '.', 'mindm', \"'\", 'feel', 'sligh', '.', '.', 'dino', '!', '==', '?', 'hello', ',', 'mr.', ',', '01/20/2020', 'https://www.naxi.rs/hyp', 'usa', 'u.s.a.', 'u']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(\"\")\n",
    "name = 'txt3'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.pipeline()\n",
    "\n",
    "#print(tokenizer.rough_tokens)\n",
    "print(tokenizer.tokens)\n",
    "tokenizer.stem_tokens()\n",
    "print(tokenizer.tokens)\n",
    "# for s in tokenizer.sentences:\n",
    "#     print([s])\n",
    "# print(tokenizer.vocabulary)\n",
    "# print(tokenizer.word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaabb\n"
     ]
    }
   ],
   "source": [
    "s = \"AaAbB\"\n",
    "s = s.lower()\n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp_p1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14664053d37e575f017670323f8571077f235bd162d1842d904390d31d3de60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
