{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from NLP import *\n",
    "from lexicons import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "b = ['a', 'a', 'b', 'c', 'a', 'A']\n",
    "new = list(set(b))\n",
    "new.sort()\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "print(tokenizer.text)\n",
    "print(tokenizer.rough_tokens)\n",
    "print(tokenizer.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.sentence_split()\n",
    "sentences = tokenizer.sentences\n",
    "print(text)\n",
    "for sentence in sentences:\n",
    "    print([sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabularies and word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "# print(tokenizer.word_frequency)\n",
    "print(tokenizer.vocabulary)\n",
    "# print(tokenizer.word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'peopl', 'ceas', 'pirat', 'file', 'when', 'those', 'same', 'peopl', 'ar', 'not', 'paid', 'for', 'their', 'particip', 'in', 'veri', 'lucr', 'network', 'scheme', 'network', 'ordinari', 'peopl', 'ar', 'relentlessli', 'spi', 'on', 'and', 'not', 'compens', 'for', 'inform', 'taken', 'from', 'them', 'while', 'i', 'would', 'like', 'to', 'see', 'everyon', 'eventu', 'pai', 'for', 'music', 'and', 'the', 'like', 'i', 'would', 'not', 'ask', 'for', 'it', 'until', 'there', 'is', 'reciproc', 'do', 'not', 'like', 'her', 'do', 'not', 'be.', 'example@gmail.com', 'multipl']\n",
      "['would', 'unfair', 'demand', 'people', 'cease', 'pirating', 'files', 'people', 'paid', 'participation', 'lucrative', 'network', 'schemes', 'networking', 'ordinary', 'people', 'relentlessly', 'spied', 'compensated', 'information', 'taken', 'would', 'like', 'see', 'everyone', 'eventually', 'pay', 'music', 'like', 'would', 'ask', 'reciprocity', 'liking', 'be.', 'example@gmail.com', 'multiplications']\n",
      "['would', 'unfair', 'demand', 'peopl', 'ceas', 'pirat', 'file', 'peopl', 'paid', 'particip', 'lucr', 'network', 'scheme', 'network', 'ordinari', 'peopl', 'relentlessli', 'spi', 'compens', 'inform', 'taken', 'would', 'like', 'see', 'everyon', 'eventu', 'pai', 'music', 'like', 'would', 'ask', 'reciproc', 'like', 'be.', 'example@gmail.com', 'multipl']\n"
     ]
    }
   ],
   "source": [
    "# import pprint\n",
    "# pprint.pprint(\"\")\n",
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "# print(tokenizer.rough_tokens)\n",
    "# print(tokenizer.dirty_tokens)\n",
    "# print(tokenizer.tokens)\n",
    "print(tokenizer.stemmed_tokens)\n",
    "tokenizer.remove_stopwords()\n",
    "print(tokenizer.pruned_tokens)\n",
    "tokenizer.stem_tokens(token_type=\"pruned\")\n",
    "print(tokenizer.stemmed_tokens)\n",
    "# tokenizer.sentence_split()\n",
    "# for s in tokenizer.sentences:\n",
    "#     print([s])\n",
    "# tokenizer.make_vocabulary()\n",
    "# print(tokenizer.vocabulary)\n",
    "# tokenizer.make_vocabulary(token_type=\"stemmed\")\n",
    "# print(tokenizer.vocabulary)\n",
    "\n",
    "#wf = tokenizer.get_word_frequencies(token_type=\"stemmed\")\n",
    "#print(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \"here\", \"than\"} \n"
     ]
    }
   ],
   "source": [
    "txt = get_text('txt5')\n",
    "print(fun1(txt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp_p1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14664053d37e575f017670323f8571077f235bd162d1842d904390d31d3de60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
