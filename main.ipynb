{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from NLP import *\n",
    "from NLP_ML import *\n",
    "from lexicons import *\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt5'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "print(tokenizer.text)\n",
    "print(tokenizer.rough_tokens)\n",
    "print(tokenizer.tokens)\n",
    "print(tokenizer.pruned_tokens)\n",
    "tokenizer.make_vocabulary(token_type = \"stemmed\")\n",
    "print(tokenizer.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.sentence_split()\n",
    "sentences = tokenizer.sentences\n",
    "print(text)\n",
    "for sentence in sentences:\n",
    "    print([sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabularies and word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "# print(tokenizer.word_frequency)\n",
    "print(tokenizer.vocabulary)\n",
    "# print(tokenizer.word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# pprint.pprint(\"\")\n",
    "name = 'txt3'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "# print(tokenizer.rough_tokens)\n",
    "# print(tokenizer.dirty_tokens)\n",
    "# print(tokenizer.tokens)\n",
    "# tokenizer.sentence_split()\n",
    "# for s in tokenizer.sentences:\n",
    "#     print([s])\n",
    "# tokenizer.make_vocabulary()\n",
    "# print(tokenizer.vocabulary)\n",
    "tokenizer.make_vocabulary(token_type=\"clean\")\n",
    "print(tokenizer.vocabulary)\n",
    "tokenizer.make_vocabulary(token_type=\"stemmed\")\n",
    "print(tokenizer.vocabulary)\n",
    "tokenizer.make_vocabulary(token_type=\"pruned\")\n",
    "print(tokenizer.vocabulary)\n",
    "\n",
    "#wf = tokenizer.get_word_frequencies(token_type=\"stemmed\")\n",
    "#print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitting using Logistical Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method make_feature in module NLP_ML:\n",
      "\n",
      "make_feature(sample_list) method of NLP_ML.NLP_ML instance\n",
      "    Function for feature compilation. To see the composition of sample list\n",
      "    look at get_samples function.\n",
      "    Features:\n",
      "    F0: Is punctuation a period?\n",
      "    F1: Is previous character lower_case?\n",
      "    F2: Is previous character upper_case?\n",
      "    F3: Is previous character number?\n",
      "    F4: Is next character letter?\n",
      "    F5: Is next character number?\n",
      "    F6: Is next character whitespace?\n",
      "    F7: Is previous token an abbreviation?\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "m = NLP_ML()\n",
    "dataset = get_text('trainset_text')\n",
    "m.make_dataset(dataset)\n",
    "print(help(m.make_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = NLP(get_text('trainset_text'))\n",
    "p.sentence_split()\n",
    "#print(p.rough_tokens)\n",
    "for sentence in p.sentences:\n",
    "    print([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '0' '0' '1' '1' '1' '1' '1' '1' '0' '0' '1' '0']\n",
      "[[0.29224788 0.70775212]\n",
      " [0.93637777 0.06362223]\n",
      " [0.7063189  0.2936811 ]\n",
      " [0.29224788 0.70775212]\n",
      " [0.29224788 0.70775212]\n",
      " [0.29224788 0.70775212]\n",
      " [0.29224788 0.70775212]\n",
      " [0.29224788 0.70775212]\n",
      " [0.29224788 0.70775212]\n",
      " [0.7063189  0.2936811 ]\n",
      " [0.7063189  0.2936811 ]\n",
      " [0.37496988 0.62503012]\n",
      " [0.82581031 0.17418969]]\n",
      "['thanks to them (not me) that the dataset is available.']\n",
      "['The include attachments, e.g. and been deleted.']\n",
      "['The train-test split procedure is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model.']\n",
      "['It is a fast and easy procedure to perform, the results of which allow you to compare the performance of machine learning algorithms for your predictive modeling problem.']\n",
      "['Although simple to use and interpret, there are times when the procedure should not be used, such as when you have a small dataset and situations where additional configuration is required, such as when it is used for classification and the dataset is not balanced.']\n",
      "['In this tutorial, you will discover how to evaluate machine learning models using the train-test split.']\n",
      "['Hello world.']\n",
      "['Hello world again Mr. Dinno on fig. 4!']\n",
      "[\"It's me.\"]\n",
      "['thanks to them (not me) that the dataset is available.']\n",
      "['The include attachments, e.g. and been deleted.']\n",
      "['The train-test split procedure is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model.']\n",
      "['It is a fast and easy procedure to perform, the results of which allow you to compare the performance of machine learning algorithms for your predictive modeling problem.']\n",
      "['Although simple to use and interpret, there are times when the procedure should not be used, such as when you have a small dataset and situations where additional configuration is required, such as when it is used for classification and the dataset is not balanced.']\n",
      "['In this tutorial, you will discover how to evaluate machine learning models using the train-test split.']\n",
      "['Hello world.']\n",
      "['Hello world again Mr. Dinno on fig. 4!']\n",
      "[\"It's me.\"]\n"
     ]
    }
   ],
   "source": [
    "txt = \"Ja M.r. Dinno imam ## pet goid? di .   \\n !  ..  \"\n",
    "txt = \"Mr. Hello world. Am I a human? dinno $2.5 koluh.\"\n",
    "txt = get_text('test')\n",
    "m = NLP_ML()\n",
    "sentences = m.sentence_split_ml(txt)\n",
    "for sentence in sentences:\n",
    "    print([sentence])\n",
    "# m.standardize_text(txt)\n",
    "# txt = m.stand_text\n",
    "# samples, features, targets = m.make_dataset(txt)\n",
    "# for sample, feature, target in zip(samples, features, targets):\n",
    "#     print(sample)\n",
    "#     print(feature)\n",
    "#     print(target)\n",
    "p = NLP(txt)\n",
    "p.sentence_split()\n",
    "#print(p.rough_tokens)\n",
    "for sentence in p.sentences:\n",
    "    print([sentence])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp_p1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14664053d37e575f017670323f8571077f235bd162d1842d904390d31d3de60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
