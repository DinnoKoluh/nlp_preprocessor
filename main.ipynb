{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from NLP import *\n",
    "from lexicons import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "b = ['a', 'a', 'b', 'c', 'a', 'A']\n",
    "new = list(set(b))\n",
    "new.sort()\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "print(tokenizer.text)\n",
    "print(tokenizer.rough_tokens)\n",
    "print(tokenizer.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "#tokenizer.tokenize()\n",
    "tokenizer.sentence_split()\n",
    "sentences = tokenizer.sentences\n",
    "print(text)\n",
    "for sentence in sentences:\n",
    "    print([sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabularies and word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "tokenizer.expand_clitics()\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "# print(tokenizer.word_frequency)\n",
    "tokenizer.clean_vocabulary()\n",
    "print(tokenizer.vocabulary)\n",
    "# print(tokenizer.word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'people', 'cease', 'pirating', 'files', 'when', 'those', 'same', 'people', \"aren't\", 'paid', 'for', 'their', 'participation', 'in', 'very', 'lucrative', 'network', 'schemes', 'networking', '.', 'Ordinary', 'people', 'are', 'relentlessly', 'spied', 'on', ',', 'and', 'not', 'compensated', 'for', 'information', 'taken', 'from', 'them', '.', 'While', \"I'd\", 'like', 'to', 'see', 'everyone', 'eventually', 'pay', 'for', 'music', 'and', 'the', 'like', ',', \"I'd\", 'not', 'ask', 'for', 'it', 'until', \"there's\", 'reciprocity', '.', \"Don't\", 'liking', 'her', '.', \"don't\", ',', 'be.', ':', 'example@gmail.com', 'Multiplications']\n",
      "['it', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'people', 'cease', 'pirating', 'files', 'when', 'those', 'same', 'people', 'are', 'not', 'paid', 'for', 'their', 'participation', 'in', 'very', 'lucrative', 'network', 'schemes', 'networking', 'ordinary', 'people', 'are', 'relentlessly', 'spied', 'on', 'and', 'not', 'compensated', 'for', 'information', 'taken', 'from', 'them', 'while', 'i', 'would', 'like', 'to', 'see', 'everyone', 'eventually', 'pay', 'for', 'music', 'and', 'the', 'like', 'i', 'would', 'not', 'ask', 'for', 'it', 'until', 'there', 'is', 'reciprocity', 'do', 'not', 'liking', 'her', 'do', 'not', 'be.', 'example@gmail.com', 'multiplications']\n",
      "['it', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'peopl', 'ceas', 'pirat', 'file', 'when', 'those', 'same', 'peopl', 'ar', 'not', 'paid', 'for', 'their', 'particip', 'in', 'veri', 'lucr', 'network', 'scheme', 'network', 'ordinari', 'peopl', 'ar', 'relentlessli', 'spi', 'on', 'and', 'not', 'compens', 'for', 'inform', 'taken', 'from', 'them', 'while', 'i', 'would', 'like', 'to', 'see', 'everyon', 'eventu', 'pai', 'for', 'music', 'and', 'the', 'like', 'i', 'would', 'not', 'ask', 'for', 'it', 'until', 'there', 'is', 'reciproc', 'do', 'not', 'like', 'her', 'do', 'not', 'be.', 'example@gmail.com', 'multipl']\n",
      "[\"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes networking.\"]\n",
      "['Ordinary people are relentlessly spied on, and not compensated for information taken from them.']\n",
      "[\"While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\"]\n",
      "[\"Don't liking her.\"]\n"
     ]
    }
   ],
   "source": [
    "# import pprint\n",
    "# pprint.pprint(\"\")\n",
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "# print(tokenizer.rough_tokens)\n",
    "print(tokenizer.dirty_tokens)\n",
    "print(tokenizer.tokens)\n",
    "print(tokenizer.stemmed_tokens)\n",
    "tokenizer.sentence_split()\n",
    "for s in tokenizer.sentences:\n",
    "    print([s])\n",
    "# tokenizer.make_vocabulary()\n",
    "# print(tokenizer.vocabulary)\n",
    "# tokenizer.make_vocabulary(token_type=\"stemmed\")\n",
    "# print(tokenizer.vocabulary)\n",
    "\n",
    "#wf = tokenizer.get_word_frequencies(token_type=\"stemmed\")\n",
    "#print(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mi\n"
     ]
    }
   ],
   "source": [
    "p = PorterStemmer()\n",
    "word = \"network\"\n",
    "word = \"mis\"\n",
    "print(p.stem(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp_p1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14664053d37e575f017670323f8571077f235bd162d1842d904390d31d3de60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
