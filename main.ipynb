{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from NLP import *\n",
    "from SentenceSplitterML import *\n",
    "from TokenizerML import *\n",
    "from lexicons import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'test'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "print(tokenizer.text)\n",
    "print(tokenizer.rough_tokens)\n",
    "print(tokenizer.dirty_tokens)\n",
    "print(tokenizer.tokens)\n",
    "print(tokenizer.stemmed_tokens)\n",
    "print(tokenizer.pruned_tokens)\n",
    "tokenizer.make_vocabulary(token_type = \"stemmed\")\n",
    "print(tokenizer.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.sentence_split()\n",
    "sentences = tokenizer.sentences\n",
    "print(text)\n",
    "for sentence in sentences:\n",
    "    print([sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabularies and word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'txt4'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "# print(tokenizer.word_frequency)\n",
    "print(tokenizer.vocabulary)\n",
    "# print(tokenizer.word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# pprint.pprint(\"\")\n",
    "name = 'txt3'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "# print(tokenizer.rough_tokens)\n",
    "# print(tokenizer.dirty_tokens)\n",
    "# print(tokenizer.tokens)\n",
    "# tokenizer.sentence_split()\n",
    "# for s in tokenizer.sentences:\n",
    "#     print([s])\n",
    "# tokenizer.make_vocabulary()\n",
    "# print(tokenizer.vocabulary)\n",
    "tokenizer.make_vocabulary(token_type=\"clean\")\n",
    "print(tokenizer.vocabulary)\n",
    "tokenizer.make_vocabulary(token_type=\"stemmed\")\n",
    "print(tokenizer.vocabulary)\n",
    "tokenizer.make_vocabulary(token_type=\"pruned\")\n",
    "print(tokenizer.vocabulary)\n",
    "\n",
    "#wf = tokenizer.get_word_frequencies(token_type=\"stemmed\")\n",
    "#print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method get_features in module TokenizerML:\n",
      "\n",
      "get_features(sample, index) method of TokenizerML.TokenizerML instance\n",
      "    Function for feature compilation. To see the composition of sample list\n",
      "    look at get_samples function.\n",
      "    Features:\n",
      "    F0: Is character a letter?\n",
      "    F1: Is character a number?\n",
      "    F2: Is character a whitespace?\n",
      "    F3: Is character between two alphanumeric values?\n",
      "    F4: Is character a punctuation sign?\n",
      "    F5: Is character next to a number?\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "m = TokenizerML()\n",
    "dataset = get_text('test')\n",
    "features, targets = m.make_dataset(dataset)\n",
    "print(help(m.get_features))\n",
    "# for s, f, t in zip(dataset, features, targets):\n",
    "#     print(s + \" \" + str(f) + \" => \" + str([t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can't find Mr. Nobody N. in New York City. Since 22/11/1996, U.S.A. and United States\n",
      "the only way is at mr_nobody@gmail.com (or at his website: www.mr_nobody.com).\n",
      "\n",
      "Invalid of the form user@enron.com., whenever possible (i.e., recipient is specified \n",
      "in some parse-able format like \"Doe, John\" or \"Mary K. Smith\" at http://www.stanford.edu). \n",
      "But 10.8$ is enough for us! I drove 50 m.p.h. and got a speeding ticket? Not possible!!\n",
      "['You', 'can', \"'\", 't', 'find', 'Mr', '.', 'Nobody', 'N', '.', 'in', 'New', 'York', 'City', '.', 'Since', '22/11/1996,', 'U', '.', 'S', '.', 'A', '.', 'and', 'United', 'States', 'the', 'only', 'way', 'is', 'at', 'mr', '_', 'nobody', '@', 'gmail', '.', 'com', '(', 'or', 'at', 'his', 'website', ':', 'www', '.', 'mr', '_', 'nobody', '.', 'com', ')', '.', 'I', 'nvalid ', 'o', 'f ', 't', 'he ', 'f', 'orm ', 'u', 'ser@', 'e', 'nron.', 'c', 'om.', ',', 'w', 'henever ', 'p', 'ossible ', '(', 'i', '.', 'e', '.', ',', 'r', 'ecipient ', 'i', 's ', 's', 'pecified ', 'i', 'n', ' s', 'o', 'me p', 'a', 'rse-a', 'b', 'le f', 'o', 'rmat l', 'i', 'ke \"', 'D', 'o', 'e, ', 'J', 'o', 'hn\" ', 'o', 'r', ' \"', 'M', 'a', 'ry K', '.', ' ', 'S', 'm', 'ith\" ', 'a', 't', ' h', 't', 'tp:/', '/', 'w', 'w', 'w.s', 't', 'anford.e', 'd', 'u).', 'B', 'u', 't', ' 10', '.', '8$ is', 'en', 'o', 'ugh fo', 'r', ' us', '!', ' I', 'd', 'r', 'o', 've 50', 'm.', 'p', '.', 'h', '.', 'a', 'n', 'd', ' go', 't', ' a ', 's', 'p', 'e', 'eding ti', 'c', 'ket? N', 'o', 't', ' po', 's']\n"
     ]
    }
   ],
   "source": [
    "name = 'test'\n",
    "text = get_text(name)\n",
    "print(text)\n",
    "mltokenizer = TokenizerML()\n",
    "\n",
    "tokens = mltokenizer.tokenize_ml(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'test'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "print(tokenizer.text)\n",
    "print(tokenizer.rough_tokens)\n",
    "print(tokenizer.dirty_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitting using Logistical Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SentenceSplitterML()\n",
    "dataset = get_text('trainset_text')\n",
    "m.make_dataset(dataset)\n",
    "print(help(m.get_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = NLP(get_text('trainset_text'))\n",
    "p.sentence_split()\n",
    "#print(p.rough_tokens)\n",
    "for sentence in p.sentences:\n",
    "    print([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Ja M.r. Dinno imam ## pet goid? di .   \\n !  ..  \"\n",
    "txt = \"Mr. Hello world. Am I a human? dinno $2.5 koluh.\"\n",
    "txt = get_text('test')\n",
    "m = SentenceSplitterML()\n",
    "sentences = m.sentence_split_ml(txt)\n",
    "for sentence in sentences:\n",
    "    print([sentence])\n",
    "# m.standardize_text(txt)\n",
    "# txt = m.stand_text\n",
    "# samples, features, targets = m.make_dataset(txt)\n",
    "# for sample, feature, target in zip(samples, features, targets):\n",
    "#     print(sample)\n",
    "#     print(feature)\n",
    "#     print(target)\n",
    "p = NLP(txt)\n",
    "p.sentence_split()\n",
    "#print(p.rough_tokens)\n",
    "for sentence in p.sentences:\n",
    "    print([sentence])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp_p1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14664053d37e575f017670323f8571077f235bd162d1842d904390d31d3de60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
