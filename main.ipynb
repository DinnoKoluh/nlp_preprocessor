{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from NLP import *\n",
    "from SentenceSplitterML import *\n",
    "from TokenizerML import *\n",
    "from lexicons import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can't find Mr. Nobody N. in New York City. Since 22/11/1996, \n",
      "the only way is at mr_nobody@gmail.com (or at his website: www.mr_nobody.com).\n",
      "['You', \"can't\", 'find', 'Mr.', 'Nobody', 'N.', 'in', 'New', 'York', 'City.', 'Since', '22/11/1996,', 'the', 'only', 'way', 'is', 'at', 'mr_nobody@gmail.com', '(or', 'at', 'his', 'website:', 'www.mr_nobody.com).']\n",
      "['You', \"can't\", 'find', 'Mr.', 'Nobody', 'N.', 'in', 'New', 'York', 'City', '.', 'Since', '22/11/1996', ',', 'the', 'only', 'way', 'is', 'at', 'mr_nobody@gmail.com', '(', 'or', 'at', 'his', 'website', ':', 'www.mr_nobody.com', '.)']\n",
      "['you', 'cannot', 'find', 'mr.', 'nobody', 'n.', 'in', 'new york city', 'since', '22/11/1996', 'the', 'only', 'way', 'is', 'at', 'mr_nobody@gmail.com', 'or', 'at', 'his', 'website', 'www.mr_nobody.com']\n",
      "['you', 'cannot', 'find', 'mr.', 'nobodi', 'n.', 'in', 'new york c', 'sinc', '22/11/1996', 'the', 'onli', 'wai', 'is', 'at', 'mr_nobody@gmail.com', 'or', 'at', 'hi', 'websit', 'www.mr_nobody.com']\n",
      "['cannot', 'find', 'mr.', 'nobody', 'n.', 'new york city', 'since', '22/11/1996', 'way', 'mr_nobody@gmail.com', 'website', 'www.mr_nobody.com']\n",
      "['22/11/1996', 'at', 'cannot', 'find', 'hi', 'in', 'is', 'mr.', 'mr_nobody@gmail.com', 'n.', 'new york c', 'nobodi', 'onli', 'or', 'sinc', 'the', 'wai', 'websit', 'www.mr_nobody.com', 'you']\n"
     ]
    }
   ],
   "source": [
    "name = 'test_t'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.tokenize()\n",
    "print(tokenizer.text)\n",
    "print(tokenizer.rough_tokens)\n",
    "print(tokenizer.dirty_tokens)\n",
    "print(tokenizer.tokens)\n",
    "print(tokenizer.stemmed_tokens)\n",
    "print(tokenizer.pruned_tokens)\n",
    "tokenizer.make_vocabulary(token_type = \"stemmed\")\n",
    "print(tokenizer.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-based sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would be unfair to demand that people cease pirating files\n",
      " when those same people aren't paid for their participation in\n",
      "  very lucrative network schemes networking. Ordinary people are relentlessly\n",
      "   spied on, and not compensated for information taken from them. \n",
      "While I'd like to see everyone eventually pay for music and the like, \n",
      "I'd not ask for it until there's reciprocity. Don't liking her.\n",
      "don't,be.:example@gmail.com Multiplications\n",
      "Mr. Winston Churcil is not a good person. But 10.8$ is enoguht for us.\n",
      "I drove 50 m.p.h. and got a speeding ticket.\n",
      "[\"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes networking.\"]\n",
      "['Ordinary people are relentlessly spied on, and not compensated for information taken from them.']\n",
      "[\"While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\"]\n",
      "[\"Don't liking her.\"]\n",
      "[\"don't,be.:example@gmail.com Multiplications Mr. Winston Churcil is not a good person.\"]\n",
      "['But 10.8$ is enoguht for us.']\n",
      "['I drove 50 m.p.h. and got a speeding ticket.']\n"
     ]
    }
   ],
   "source": [
    "name = 'test_ss'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.sentence_split()\n",
    "sentences = tokenizer.sentences\n",
    "print(text)\n",
    "for sentence in sentences:\n",
    "    print([sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabularies and word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'people', 'cease', 'pirating', 'files', 'when', 'those', 'same', 'people', 'are', 'not', 'paid', 'for', 'their', 'participation', 'in', 'very', 'lucrative', 'network', 'schemes', 'networking', 'ordinary', 'people', 'are', 'relentlessly', 'spied', 'on', 'and', 'not', 'compensated', 'for', 'information', 'taken', 'from', 'them', 'while', 'i', 'would', 'like', 'to', 'see', 'everyone', 'eventually', 'pay', 'for', 'music', 'and', 'the', 'like', 'i', 'would', 'not', 'ask', 'for', 'it', 'until', 'there', 'is', 'reciprocity', 'do', 'not', 'liking', 'her', 'do', 'not', 'be.', 'example@gmail.com', 'multiplications', 'mr.', 'winston', 'churcil', 'is', 'not', 'a', 'good', 'person', 'but', '10.8$', 'is', 'enoguht', 'for', 'us', 'i', 'drove', '50', 'm.p.h.', 'and', 'got', 'a', 'speeding', 'ticket']\n",
      "['10.8$', '50', 'a', 'and', 'are', 'ask', 'be', 'be.', 'but', 'cease', 'churcil', 'compensated', 'demand', 'do', 'drove', 'enoguht', 'eventually', 'everyone', 'example@gmail.com', 'files', 'for', 'from', 'good', 'got', 'her', 'i', 'in', 'information', 'is', 'it', 'like', 'liking', 'lucrative', 'm.p.h.', 'mr.', 'multiplications', 'music', 'network', 'networking', 'not', 'on', 'ordinary', 'paid', 'participation', 'pay', 'people', 'person', 'pirating', 'reciprocity', 'relentlessly', 'same', 'schemes', 'see', 'speeding', 'spied', 'taken', 'that', 'the', 'their', 'them', 'there', 'those', 'ticket', 'to', 'unfair', 'until', 'us', 'very', 'when', 'while', 'winston', 'would']\n",
      "{'.': 7, 'for': 5, 'people': 3, ',': 3, 'and': 3, 'not': 3, 'to': 2, \"I'd\": 2, 'like': 2, 'is': 2, 'a': 2, 'It': 1, 'would': 1, 'be': 1, 'unfair': 1, 'demand': 1, 'that': 1, 'cease': 1, 'pirating': 1, 'files': 1, 'when': 1, 'those': 1, 'same': 1, \"aren't\": 1, 'paid': 1, 'their': 1, 'participation': 1, 'in': 1, 'very': 1, 'lucrative': 1, 'network': 1, 'schemes': 1, 'networking': 1, 'Ordinary': 1, 'are': 1, 'relentlessly': 1, 'spied': 1, 'on': 1, 'compensated': 1, 'information': 1, 'taken': 1, 'from': 1, 'them': 1, 'While': 1, 'see': 1, 'everyone': 1, 'eventually': 1, 'pay': 1, 'music': 1, 'the': 1, 'ask': 1, 'it': 1, 'until': 1, \"there's\": 1, 'reciprocity': 1, \"Don't\": 1, 'liking': 1, 'her': 1, \"don't\": 1, 'be.': 1, ':': 1, 'example@gmail.com': 1, 'Multiplications': 1, 'Mr.': 1, 'Winston': 1, 'Churcil': 1, 'good': 1, 'person': 1, 'But': 1, '10.8$': 1, 'enoguht': 1, 'us': 1, 'I': 1, 'drove': 1, '50': 1, 'm.p.h.': 1, 'got': 1, 'speeding': 1, 'ticket': 1}\n"
     ]
    }
   ],
   "source": [
    "name = 'test_ss'\n",
    "text = get_text(name)\n",
    "tokenizer = NLP(text)\n",
    "tokenizer.make_vocabulary()\n",
    "print(tokenizer.tokens)\n",
    "print(tokenizer.vocabulary)\n",
    "print(tokenizer.get_word_frequencies(token_type = \"dirty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method get_features in module TokenizerML:\n",
      "\n",
      "get_features(sample, index) method of TokenizerML.TokenizerML instance\n",
      "    Function for feature compilation. To see the composition of sample list\n",
      "    look at get_samples function.\n",
      "    Features:\n",
      "    F0: Is character a letter?\n",
      "    F1: Is character a number?\n",
      "    F2: Is character a whitespace?\n",
      "    F3: Is character between two alphanumeric values?\n",
      "    F4: Is character a punctuation sign?\n",
      "    F5: Is character next to a number?\n",
      "    F6: Is character @?\n",
      "    F7: Is character a punctuation sign between two alphanumeric values?\n",
      "\n",
      "None\n",
      "Y [1, 0, 0, 0, 0, 0, 0, 0] => [0]\n",
      "o [1, 0, 0, 1, 0, 0, 0, 0] => [0]\n",
      "u [1, 0, 0, 0, 0, 0, 0, 0] => [0]\n",
      "  [0, 0, 1, 1, 0, 0, 0, 0] => [1]\n",
      "c [1, 0, 0, 0, 0, 0, 0, 0] => [0]\n",
      "' [0, 0, 0, 1, 1, 0, 0, 1] => [0]\n",
      "t [1, 0, 0, 0, 0, 0, 0, 0] => [0]\n",
      "  [0, 0, 1, 1, 0, 0, 0, 0] => [1]\n",
      "f [1, 0, 0, 0, 0, 0, 0, 0] => [0]\n",
      "d [1, 0, 0, 0, 0, 0, 0, 0] => [0]\n",
      "  [0, 0, 1, 1, 0, 0, 0, 0] => [1]\n",
      "M [1, 0, 0, 0, 0, 0, 0, 0] => [0]\n",
      "r [1, 0, 0, 0, 0, 0, 0, 0] => [0]\n",
      ". [0, 0, 0, 0, 1, 0, 0, 0] => [0]\n",
      "  [0, 0, 1, 0, 0, 0, 0, 0] => [1]\n"
     ]
    }
   ],
   "source": [
    "m = TokenizerML()\n",
    "dataset = get_text('trainset_t')\n",
    "features, targets = m.make_dataset(dataset)\n",
    "print(help(m.get_features))\n",
    "for s, f, t in zip(m.stand_text[0:15], features[0:15], targets[0:15]):\n",
    "    print(s + \" \" + str(f) + \" => \" + str([t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can't find Mr. Nobody N. in New York City. Since 22/11/1996, \n",
      "the only way is at mr_nobody@gmail.com (or at his website: www.mr_nobody.com).\n",
      "Y [1, 0, 0, 0, 0, 0, 0, 0] => [0] with p = [0.98046019 0.01953981]\n",
      "o [1, 0, 0, 1, 0, 0, 0, 0] => [0] with p = [0.98212894 0.01787106]\n",
      "u [1, 0, 0, 0, 0, 0, 0, 0] => [0] with p = [0.98046019 0.01953981]\n",
      "  [0, 0, 1, 1, 0, 0, 0, 0] => [1] with p = [0.02317899 0.97682101]\n",
      "c [1, 0, 0, 0, 0, 0, 0, 0] => [0] with p = [0.98046019 0.01953981]\n",
      "a [1, 0, 0, 1, 0, 0, 0, 0] => [0] with p = [0.98212894 0.01787106]\n",
      "n [1, 0, 0, 0, 0, 0, 0, 0] => [0] with p = [0.98046019 0.01953981]\n",
      "' [0, 0, 0, 1, 1, 0, 0, 1] => [0] with p = [0.57554159 0.42445841]\n",
      "t [1, 0, 0, 0, 0, 0, 0, 0] => [0] with p = [0.98046019 0.01953981]\n",
      "  [0, 0, 1, 1, 0, 0, 0, 0] => [1] with p = [0.02317899 0.97682101]\n",
      "f [1, 0, 0, 0, 0, 0, 0, 0] => [0] with p = [0.98046019 0.01953981]\n",
      "i [1, 0, 0, 1, 0, 0, 0, 0] => [0] with p = [0.98212894 0.01787106]\n",
      "n [1, 0, 0, 1, 0, 0, 0, 0] => [0] with p = [0.98212894 0.01787106]\n",
      "d [1, 0, 0, 0, 0, 0, 0, 0] => [0] with p = [0.98046019 0.01953981]\n",
      "  [0, 0, 1, 1, 0, 0, 0, 0] => [1] with p = [0.02317899 0.97682101]\n",
      "['You', \"can't\", 'find', 'Mr', '.', 'Nobody', 'N', '.', 'in', 'New', 'York', 'City', '.', 'Since', '22/11/1996', ',', 'the', 'only', 'way', 'is', 'at', 'mr_nobody@gmail.com', '(', 'or', 'at', 'his', 'website', ':', 'www.mr_nobody.com', ')', '.', '']\n"
     ]
    }
   ],
   "source": [
    "name = 'test_t'\n",
    "text = get_text(name)\n",
    "print(text)\n",
    "mltokenizer = TokenizerML()\n",
    "tokens, features, targets, probs = mltokenizer.tokenize_ml(text)\n",
    "for s, f, t, p in zip(mltokenizer.stand_text[0:15], features[0:15], targets, probs[0:15]):\n",
    "    print(s + \" \" + str(f) + \" => \" + str([t]) + \" with p = \" + str(p))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence splitting using Logistical Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method get_features in module SentenceSplitterML:\n",
      "\n",
      "get_features(sample_list) method of SentenceSplitterML.SentenceSplitterML instance\n",
      "    Function for feature compilation. To see the composition of sample list\n",
      "    look at get_samples function.\n",
      "    Features:\n",
      "    F0: Is punctuation a period?\n",
      "    F1: Is previous character lower_case?\n",
      "    F2: Is previous character upper_case?\n",
      "    F3: Is previous character number?\n",
      "    F4: Is next character letter?\n",
      "    F5: Is next character number?\n",
      "    F6: Is next character whitespace?\n",
      "    F7: Is previous token an abbreviation?\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "m = SentenceSplitterML()\n",
    "dataset = get_text('trainset_ss')\n",
    "m.make_dataset(dataset)\n",
    "print(help(m.get_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 0 0 1 0 1 0 0 0 0]\n",
      "[[0.29224788 0.70775212]\n",
      " [0.29224788 0.70775212]\n",
      " [0.29224788 0.70775212]\n",
      " [0.29224788 0.70775212]\n",
      " [0.82581031 0.17418969]\n",
      " [0.93637777 0.06362223]\n",
      " [0.7063189  0.2936811 ]\n",
      " [0.29224788 0.70775212]\n",
      " [0.92868398 0.07131602]\n",
      " [0.29224788 0.70775212]\n",
      " [0.93637777 0.06362223]\n",
      " [0.93637777 0.06362223]\n",
      " [0.7063189  0.2936811 ]\n",
      " [0.82581031 0.17418969]]\n",
      "[\"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes networking.\"]\n",
      "['Ordinary people are relentlessly spied on, and not compensated for information taken from them.']\n",
      "[\"While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\"]\n",
      "[\"Don't liking her.\"]\n",
      "[\"don't,be.:example@gmail.com Multiplications Mr. Winston Churcil is not a good person.\"]\n",
      "['But 10.8$ is enoguht for us.']\n",
      "['I drove 50 m.p.h. and got a speeding ticket.']\n"
     ]
    }
   ],
   "source": [
    "txt = get_text('test_ss')\n",
    "m = SentenceSplitterML()\n",
    "sentences = m.sentence_split_ml(txt)\n",
    "for sentence in sentences:\n",
    "    print([sentence])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp_p1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14664053d37e575f017670323f8571077f235bd162d1842d904390d31d3de60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
